{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-gpu==1.6.3 in ./anaconda3/lib/python3.8/site-packages (1.6.3)\r\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.8/site-packages (from faiss-gpu==1.6.3) (1.20.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-gpu==1.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import faiss\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(31)\n",
    "torch.cuda.manual_seed_all(31)\n",
    "np.random.seed(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcluster import AverageMeter, UnifLabelSampler, ReassignedDataset, Kmeans, cluster_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, features, num_classes, sobel):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = features\n",
    "        # self.classifier = nn.Sequential(nn.Dropout(0.5),\n",
    "        #                     nn.Linear(256 * 6 * 6, 4096),\n",
    "        #                     nn.ReLU(inplace=True),\n",
    "        #                     nn.Dropout(0.5),\n",
    "        #                     nn.Linear(4096, 4096),\n",
    "        #                     nn.ReLU(inplace=True))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(256 * 6 * 6, 4096),\n",
    "                            nn.ReLU(inplace=True),\n",
    "\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(4096, 4096),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            \n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(4096, 256),\n",
    "                            nn.ReLU(inplace=True)\n",
    "                            \n",
    "                            )\n",
    "\n",
    "        self.top_layer = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "        if sobel:\n",
    "            grayscale = nn.Conv2d(3, 1, kernel_size=1, stride=1, padding=0)\n",
    "            grayscale.weight.data.fill_(1.0 / 3.0)\n",
    "            grayscale.bias.data.zero_()\n",
    "            sobel_filter = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1)\n",
    "            sobel_filter.weight.data[0, 0].copy_(\n",
    "                torch.FloatTensor([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "            )\n",
    "            sobel_filter.weight.data[1, 0].copy_(\n",
    "                torch.FloatTensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "            )\n",
    "            sobel_filter.bias.data.zero_()\n",
    "            self.sobel = nn.Sequential(grayscale, sobel_filter)\n",
    "            for p in self.sobel.parameters():\n",
    "                p.requires_grad = False\n",
    "        else:\n",
    "            self.sobel = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.sobel:\n",
    "            x = self.sobel(x)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        if self.top_layer:\n",
    "            x = self.top_layer(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for y, m in enumerate(self.modules()):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                for i in range(m.out_channels):\n",
    "                    m.weight.data[i].normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "CFG = {\n",
    "    '2012': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "}\n",
    "\n",
    "def make_layers_features(cfg, input_dim, bn):\n",
    "    layers = []\n",
    "    in_channels = input_dim\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=3, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v[0], kernel_size=v[1], stride=v[2], padding=v[3])\n",
    "            if bn:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v[0]), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v[0]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def alexnet(sobel=False, bn=True, out=1000):\n",
    "    dim = 2 + int(not sobel)\n",
    "    model = AlexNet(make_layers_features(CFG['2012'], dim, bn=bn), out, sobel)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): DataParallel(\n",
       "    (module): Sequential(\n",
       "      (0): Conv2d(2, 96, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU(inplace=True)\n",
       "      (11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU(inplace=True)\n",
       "      (17): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=4096, out_features=256, bias=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (top_layer): None\n",
       "  (sobel): Sequential(\n",
       "    (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = alexnet(sobel=True ,out=2)\n",
    "fd = int(model.top_layer.weight.size()[1])\n",
    "model.top_layer = None\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "tra = [transforms.Resize(256),\n",
    "           transforms.CenterCrop(224),\n",
    "           transforms.ToTensor(),\n",
    "           normalize]\n",
    "\n",
    "dataset = datasets.ImageFolder(\"./train/\", transform=transforms.Compose(tra) )\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=True,\n",
    "                                          batch_size=4,\n",
    "                                          num_workers=2,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "        filter(lambda x: x.requires_grad, model.parameters()),\n",
    "        lr=0.0005\n",
    "        # momentum=0.9,\n",
    "        # weight_decay=10**-5,\n",
    "    )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(dataloader, model, N):\n",
    "    #print('Compute features')\n",
    "    batch_time = AverageMeter()\n",
    "    end = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    # discard the label information in the dataloader\n",
    "    for i, (input_tensor, _) in enumerate(dataloader):\n",
    "        #input_var = torch.autograd.Variable(input_tensor.cuda(), volatile=True)\n",
    "        #input_var = torch.tensor( input_tensor.cuda(), requires_grad=True)\n",
    "        input_var = input_tensor.cuda().clone().detach().requires_grad_(True)\n",
    "        aux = model(input_var).data.cpu().numpy()\n",
    "\n",
    "        if i == 0:\n",
    "            features = np.zeros((N, aux.shape[1]), dtype='float32')\n",
    "\n",
    "        aux = aux.astype('float32')\n",
    "        if i < len(dataloader) - 1:\n",
    "            features[i * 4: (i + 1) * 4] = aux\n",
    "        else:\n",
    "            # special treatment for final batch\n",
    "            features[i * 4:] = aux\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # if (i % 50) == 0:\n",
    "        #     print('{0} / {1}\\t'\n",
    "        #           'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})'\n",
    "        #           .format(i, len(dataloader), batch_time=batch_time))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, model, crit, oopt, epoch):\n",
    "    \"\"\"Training of the CNN.\n",
    "        Args:\n",
    "            loader (torch.utils.data.DataLoader): Data loader\n",
    "            model (nn.Module): CNN\n",
    "            crit (torch.nn): loss\n",
    "            opt (torch.optim.SGD): optimizer for every parameters with True\n",
    "                                   requires_grad in model except top layer\n",
    "            epoch (int)\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    forward_time = AverageMeter()\n",
    "    backward_time = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    # create an optimizer for the last fc layer\n",
    "    optimizer_tl = torch.optim.SGD(\n",
    "        model.top_layer.parameters(),\n",
    "        lr=0.0005\n",
    "        #weight_decay=10**-5,\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input_tensor, target) in enumerate(loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(non_blocking = True)\n",
    "        input_var = torch.autograd.Variable(input_tensor.cuda())\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        output = model(input_var)\n",
    "        loss = crit(output, target_var)\n",
    "\n",
    "        # record loss\n",
    "        #losses.update(loss.data[0], input_tensor.size(0))\n",
    "        losses.update(loss.item(), input_tensor.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        opt.zero_grad()\n",
    "        optimizer_tl.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        optimizer_tl.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_validation(model , val_dataloader ,ctr ):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "    \n",
    "        for idx ,batch in enumerate(val_dataloader ,1):\n",
    "            inputs = batch[0].cuda()\n",
    "            target = batch[1].cuda()\n",
    "\n",
    "            op = model(inputs)\n",
    "\n",
    "            loss = ctr( op , target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Clustering' object has no attribute 'iteration_stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6d8c800d665a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mclustering_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_lists\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepcluster.py\u001b[0m in \u001b[0;36mcluster\u001b[0;34m(self, data, verbose)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m#if verbose:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepcluster.py\u001b[0m in \u001b[0;36mrun_kmeans\u001b[0;34m(x, nmb_clusters, verbose)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m#print('Cluster Lose ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m#print('k-means loss evolution: {0}'.format(losses))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Clustering' object has no attribute 'iteration_stats'"
     ]
    }
   ],
   "source": [
    "deepcluster = Kmeans(2)\n",
    "\n",
    "val_loss = []\n",
    "\n",
    "train_CNN_loss = []\n",
    "train_CLuster_loss = []\n",
    "\n",
    "for epoch in range(0, 200):\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    model.top_layer = None\n",
    "    model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "\n",
    "    features = compute_features(dataloader, model, len(dataset))\n",
    "\n",
    "    clustering_loss = deepcluster.cluster(features, verbose=True)\n",
    "\n",
    "    train_dataset = cluster_assign(deepcluster.images_lists,dataset)\n",
    "\n",
    "    # uniformly sample per target\n",
    "    sampler = UnifLabelSampler(int(1 * len(train_dataset)), deepcluster.images_lists)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=4,\n",
    "            num_workers=2,\n",
    "            sampler=sampler,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    # set last fully connected layer\n",
    "    mlp = list(model.classifier.children())\n",
    "    mlp.append(nn.ReLU(inplace=True).cuda())\n",
    "    model.classifier = nn.Sequential(*mlp)\n",
    "    model.top_layer = nn.Linear(fd, len(deepcluster.images_lists))\n",
    "    model.top_layer.weight.data.normal_(0, 0.01)\n",
    "    model.top_layer.bias.data.zero_()\n",
    "    model.top_layer.cuda()\n",
    "    end = time.time()\n",
    "    \n",
    "    # train network with clusters as pseudo-labels\n",
    "    loss = train(dataloader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print('Check Point Saved At Epoch : ' + str(epoch) )\n",
    "        f = open('./model_100' ,'wb')\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "        clus_200 = open('./cluster_list' ,'wb')\n",
    "        pickle.dump(deepcluster.images_lists, clus_200)\n",
    "    \n",
    "    print('###### Epoch : '+ str(epoch) +' #####')\n",
    "    print('Clustering loss : ' + str(clustering_loss))\n",
    "    print('ConvLoss : '+ str(loss))\n",
    "    print('#######################')\n",
    "    print()\n",
    "\n",
    "    train_CNN_loss.append(loss)\n",
    "    train_CLuster_loss.append(clustring_loss)\n",
    "\n",
    "    ### Validation \n",
    "    val_loss.append(do_validation(model , val_dataloader , criterion ) )\n",
    "\n",
    "\n",
    "f = open('./model_100' ,'wb')\n",
    "pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss( train_loss , val_loss ):\n",
    "    epochs = list( range(101) ) \n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    plt.plot(epochs, train_loss,label = \"Train Loss\")\n",
    "    plt.plot(epochs, val_loss,label = \"Validation Loss\")\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.title('Epochs Vs Loss')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_CNN_loss , val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_loss( cluster_loss ):\n",
    "    epochs = list( range(101) ) \n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    plt.plot(epochs, cluster_loss,label = \"Cluster Loss\")\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.title('Epochs Vs Loss')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_cluster_loss(train_CLuster_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(t):\n",
    "\n",
    "    data , label = t\n",
    "    print('Original Label :' + str(label) )\n",
    "    img = data.numpy()\n",
    "\n",
    "    op_img = np.zeros((224,224,3))\n",
    "    op_img.shape\n",
    "\n",
    "    for i in range(224):\n",
    "        for j in range(224):\n",
    "            col = [ img[0][i][j] , img[1][i][j] , img[2][i][j] ]\n",
    "\n",
    "            op_img[i][j] = col \n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(op_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img( dataset[6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats , dogs = 0 ,0\n",
    "\n",
    "for i in dataset:\n",
    "    if i[1] == 0:\n",
    "        cats += 1\n",
    "    else:\n",
    "        dogs += 1\n",
    "\n",
    "labels = list( ['Cats(0)' ,'Dogs(1)'] )\n",
    "values = list([cats ,dogs ])\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(labels, values, color ='maroon', width = 0.4)\n",
    " \n",
    "plt.title(' Original Data Distribution')\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deep cluster Distribution \n",
    "f = open('./clusters_list' , 'rb')\n",
    "deepcluster = pickle.load(f)\n",
    "\n",
    "clus0 = deepcluster.images_lists[0]\n",
    "clus1 = deepcluster.images_lists[1]\n",
    "\n",
    "labels = list( ['Cluster 0' , 'Cluster 1'] )\n",
    "values = list([ len(clus0)  , len(clus1) ])\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(labels, values, color ='maroon', width = 0.4)\n",
    " \n",
    "plt.title('Clusters After Training of 100 epochs')\n",
    "plt.xlabel(\"Clusters\")\n",
    "plt.ylabel(\"Samples\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cluster 0 Analysis \n",
    "\n",
    "clus0_cat = 0\n",
    "clus0_dog = 0\n",
    "\n",
    "for idx in clus0:\n",
    "    img ,label = dataset[idx]\n",
    "\n",
    "    if label == 0 :\n",
    "        clus0_cat += 1\n",
    "    else:\n",
    "        clus0_dog += 1\n",
    "\n",
    "labels = list( ['Cats' , 'Dogs'] )\n",
    "values = list([ clus0_cat , clus0_dog ])\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(labels, values, color ='maroon', width = 0.4)\n",
    " \n",
    "plt.title('Cluster 0 Class Distribution')\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Samples\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cluster 1 Analysis \n",
    "\n",
    "clus1_cat = 0\n",
    "clus1_dog = 0\n",
    "\n",
    "for idx in clus1:\n",
    "    img ,label = dataset[idx]\n",
    "\n",
    "    if label == 0 :\n",
    "        clus1_cat += 1\n",
    "    else:\n",
    "        clus1_dog += 1\n",
    "\n",
    "labels = list( ['Cats' , 'Dogs'] )\n",
    "values = list([ clus1_cat , clus1_dog ])\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(labels, values, color ='maroon', width = 0.4)\n",
    " \n",
    "plt.title('Cluster 1 Class Distribution')\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Samples\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "tra = [transforms.Resize(256),\n",
    "           transforms.CenterCrop(224),\n",
    "           transforms.ToTensor(),\n",
    "           normalize]\n",
    "\n",
    "test_set = datasets.ImageFolder('./test', transform=transforms.Compose(tra))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=1,\n",
    "                                          num_workers=2,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./model_100' , 'rb')\n",
    "final_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "final_model.eval()\n",
    "\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data = data.cuda()\n",
    "        preds = final_model(data)\n",
    "        \n",
    "        probs = torch.exp(preds).cuda()\n",
    "        prob = list(probs.numpy())\n",
    "\n",
    "        prediction = np.argmax(prob, axis=1)\n",
    "        \n",
    "        preds.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_labels = []\n",
    "zero , one = 0 ,0\n",
    "\n",
    "for i in test_set:\n",
    "    orig_labels.append(1 - i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy Score :' + str(accuracy_score(preds , orig_labels) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_mat = confusion_matrix(orig_labels, preds)\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_mat, index = ['Dogs' , 'Cats'],columns = ['Dogs' ,'Cats'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_orig = []\n",
    "dogs_orig = []\n",
    "\n",
    "incorrect_cats = []\n",
    "incorrect_dogs = []\n",
    "\n",
    "for i in range(len(pred)):\n",
    "\n",
    "    if orig_labels[i] == pred[i] and orig_labesl[i] == 1:\n",
    "        cats_orig.append(i)\n",
    "    \n",
    "    if orig_labels[i] == pred[i] and orig_labesl[i] == 0:\n",
    "        dogs_orig.append(i)\n",
    "    \n",
    "    if orig_labels[i] != pred[i] and pred[i] == 1:\n",
    "        incorrect_cats.append(i)\n",
    "    \n",
    "    if orig_labels[i] != pred[i] and pred[i] == 0:\n",
    "        incorrect_dogs.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correct(img_list,test_set):\n",
    "    final_img = []\n",
    "\n",
    "    for i in img_list:\n",
    "\n",
    "        data , label = test_set[i]\n",
    "        \n",
    "        img = data.numpy()\n",
    "\n",
    "        op_img = np.zeros((224,224,3))\n",
    "        op_img.shape\n",
    "\n",
    "        for i in range(224):\n",
    "            for j in range(224):\n",
    "                col = [ img[0][i][j] , img[1][i][j] , img[2][i][j] ]\n",
    "\n",
    "                op_img[i][j] = col \n",
    "        \n",
    "        final_img.append(op_img)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.pyplot import figure\n",
    "   \n",
    "    f, axarr = plt.subplots(1,4 ,figsize=(10,10))\n",
    "    \n",
    "    axarr[0].imshow(final_img[0])\n",
    "    axarr[1].imshow(final_img[1])\n",
    "    axarr[2].imshow(final_img[2])\n",
    "    axarr[3].imshow(final_img[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Label Cats')\n",
    "plot_correct( cats_orig[:4] , test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Label Dogs')\n",
    "plot_correct( dogs_orig[:4] , test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Incorrect predicted Cats')\n",
    "plot_correct( incorrect_cats[:4] , test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Incorrect Predicted Dogs')\n",
    "plot_correct( incorrect_dogs[:4] , test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
